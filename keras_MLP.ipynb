{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a test to creat a MLP with Keras for the Kick Off ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# data\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4869 - loss: 0.7695\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4643 - loss: 0.7490\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4377 - loss: 0.6953\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4421 - loss: 0.6646\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5152 - loss: 0.6388\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5886 - loss: 0.6193\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6862 - loss: 0.5890\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7478 - loss: 0.5628\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7655 - loss: 0.5486\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.5219\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8313 - loss: 0.5068\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8301 - loss: 0.4874\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8424 - loss: 0.4684\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8551 - loss: 0.4478\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8466 - loss: 0.4394\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8490 - loss: 0.4348\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8555 - loss: 0.4218\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8769 - loss: 0.3873\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8808 - loss: 0.3709\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8907 - loss: 0.3429\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8929 - loss: 0.3338\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8946 - loss: 0.3255\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8646 - loss: 0.3283\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8715 - loss: 0.3196\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8890 - loss: 0.2887\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8893 - loss: 0.2874\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8892 - loss: 0.2771\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8653 - loss: 0.3016\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8930 - loss: 0.2602\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8677 - loss: 0.2941\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9002 - loss: 0.2402\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8824 - loss: 0.2546\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8878 - loss: 0.2579\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8628 - loss: 0.2827\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8783 - loss: 0.2524\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8894 - loss: 0.2532\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8876 - loss: 0.2354\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8833 - loss: 0.2432\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8910 - loss: 0.2442\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8793 - loss: 0.2467\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2158\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8826 - loss: 0.2469\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8578 - loss: 0.2943\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8784 - loss: 0.2407\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9017 - loss: 0.2183\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8771 - loss: 0.2381\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8904 - loss: 0.2381\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8941 - loss: 0.2220\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8970 - loss: 0.2178\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8920 - loss: 0.2233\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8879 - loss: 0.2309\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9008 - loss: 0.2150\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8871 - loss: 0.2366\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8706 - loss: 0.2618\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8888 - loss: 0.2410\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8892 - loss: 0.2128\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8846 - loss: 0.2379\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8992 - loss: 0.2210\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8796 - loss: 0.2485\n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8874 - loss: 0.2292\n",
      "Epoch 61/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8992 - loss: 0.2012\n",
      "Epoch 62/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8960 - loss: 0.2168\n",
      "Epoch 63/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8949 - loss: 0.2250\n",
      "Epoch 64/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8874 - loss: 0.2354\n",
      "Epoch 65/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8972 - loss: 0.2262\n",
      "Epoch 66/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9025 - loss: 0.2090\n",
      "Epoch 67/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8939 - loss: 0.2150\n",
      "Epoch 68/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9000 - loss: 0.2059\n",
      "Epoch 69/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8958 - loss: 0.2095\n",
      "Epoch 70/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8804 - loss: 0.2344\n",
      "Epoch 71/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8765 - loss: 0.2204\n",
      "Epoch 72/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8820 - loss: 0.2336\n",
      "Epoch 73/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8967 - loss: 0.2170\n",
      "Epoch 74/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9058 - loss: 0.1990\n",
      "Epoch 75/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8877 - loss: 0.2159\n",
      "Epoch 76/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8883 - loss: 0.2219\n",
      "Epoch 77/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9076 - loss: 0.1926\n",
      "Epoch 78/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8782 - loss: 0.2340\n",
      "Epoch 79/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9016 - loss: 0.2006\n",
      "Epoch 80/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8981 - loss: 0.2086\n",
      "Epoch 81/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8992 - loss: 0.2031\n",
      "Epoch 82/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8866 - loss: 0.2049\n",
      "Epoch 83/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8698 - loss: 0.2376\n",
      "Epoch 84/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9089 - loss: 0.1913\n",
      "Epoch 85/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8869 - loss: 0.2146\n",
      "Epoch 86/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8857 - loss: 0.2203\n",
      "Epoch 87/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8765 - loss: 0.2310\n",
      "Epoch 88/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9027 - loss: 0.2150\n",
      "Epoch 89/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8855 - loss: 0.2193\n",
      "Epoch 90/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8941 - loss: 0.1960\n",
      "Epoch 91/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9113 - loss: 0.1963\n",
      "Epoch 92/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9110 - loss: 0.1867\n",
      "Epoch 93/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8694 - loss: 0.2280\n",
      "Epoch 94/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9015 - loss: 0.2065\n",
      "Epoch 95/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8930 - loss: 0.2196\n",
      "Epoch 96/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8907 - loss: 0.2138\n",
      "Epoch 97/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8898 - loss: 0.2130\n",
      "Epoch 98/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8711 - loss: 0.2290\n",
      "Epoch 99/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8874 - loss: 0.2326\n",
      "Epoch 100/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8810 - loss: 0.2192\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9132 - loss: 0.2018  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "#train\n",
    "history = model.fit(X_train, y_train, epochs=100, verbose=1)\n",
    "\n",
    "#evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print('loss:', loss)\n",
    "# print('accuracy:', accuracy)\n",
    "\n",
    "#plot the loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">197</span> (792.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m197\u001b[0m (792.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> (260.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65\u001b[0m (260.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> (532.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m132\u001b[0m (532.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel:\n",
    "    def __init__(self, input_dim, layers, activations, optimizer='adam', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the CustomModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - input_dim: int, the number of input features.\n",
    "        - layers: list of int, the number of neurons in each layer.\n",
    "        - activations: list of str, the activation function for each layer.\n",
    "        - optimizer: str, the optimizer to use (default is 'adam').\n",
    "        - learning_rate: float, the learning rate for the optimizer (default is 0.001).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the Sequential model based on the specified layers and activations.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add the first layer with input dimension\n",
    "        model.add(Dense(self.layers[0], input_dim=self.input_dim, activation=self.activations[0]))\n",
    "        \n",
    "        # Add the remaining layers\n",
    "        for neurons, activation in zip(self.layers[1:], self.activations[1:]):\n",
    "            model.add(Dense(neurons, activation=activation))\n",
    "        \n",
    "        # Compile the model\n",
    "        if self.optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Currently only 'adam' optimizer is supported.\")\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the model on the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: array-like, the training data.\n",
    "        - y_train: array-like, the training labels.\n",
    "        - epochs: int, the number of epochs to train (default is 10).\n",
    "        - batch_size: int, the batch size for training (default is 32).\n",
    "        \"\"\"\n",
    "        self.history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the provided test data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: array-like, the test data.\n",
    "        - y_test: array-like, the test labels.\n",
    "        \"\"\"\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, the data to make predictions on.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "\n",
    "        Parameters:\n",
    "        - filename: str, the name of the file to save the model to.\n",
    "        \"\"\"\n",
    "        self.model.save(filename)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Print the summary of the model.\n",
    "        \"\"\"\n",
    "        self.model.summary()\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"\n",
    "        Load a model from a file.\n",
    "\n",
    "        Parameters:\n",
    "        - filename: str, the name of the file to load the model from.\n",
    "        \"\"\"\n",
    "        from keras.models import load_model\n",
    "        self.model = load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6052 - loss: 1.6035\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8729 - loss: 0.2920\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8943 - loss: 0.2362\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8903 - loss: 0.2135\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9174 - loss: 0.1653\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.1203\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0530\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9942 - loss: 0.0338\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0172\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9988 - loss: 0.0146\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0085\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0082\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0058\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9993 - loss: 0.0042\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0046\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0032   \n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025   \n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0020\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0014   \n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0015   \n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0014   \n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.6370e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.1598e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0020   \n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.3007e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.8827e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0029\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0012   \n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2978e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.3670e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.1903e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.9134e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.1301e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.9999e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.9777e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.7794e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.1022e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.4535e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.5929e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.3530e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.7772e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.3579e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.6606e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011   \n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.2207e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.1820e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.4330e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.5981e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016   \n",
      "Epoch 65/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9982 - loss: 0.0034   \n",
      "Epoch 66/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9887 - loss: 0.0398\n",
      "Epoch 67/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0846\n",
      "Epoch 68/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025   \n",
      "Epoch 69/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.0776e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.0719e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.2557e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.4061e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.4842e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2962e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.4031e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 6.0448e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.9117e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.0606e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.0126e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.5148e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.4921e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.0599e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0119e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.5268e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.8803e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.1672e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4278e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.4289e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.6071e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.8106e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.5894e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.4609e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.0288e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.9955e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2695e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.3838e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.5406e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2264e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.3871e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.2145e-04\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9978 - loss: 0.0079      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9933333396911621\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One-hot encode the target arrays\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the model parameters\n",
    "    input_dim = 2  # Number of input features\n",
    "    layers = [32, 12, 10] # The last one is the number of output classes\n",
    "    activations = ['relu', 'relu', 'softmax']\n",
    "\n",
    "    # Create the custom model\n",
    "    custom_model = KerasModel(input_dim, layers, activations)\n",
    "\n",
    "    # Assume X_train, y_train, X_test, y_test are your data\n",
    "    # Train the model\n",
    "    custom_model.train(X_train, y_train_one_hot, epochs=100, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = custom_model.evaluate(X_test, y_test_one_hot)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Save the model\n",
    "    # custom_model.save_model(\"custom_model.h5\")\n",
    "\n",
    "    # Load the model\n",
    "    custom_model.load_model(\"custom_model.h5\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = custom_model.predict(X_test)\n",
    "\n",
    "    # plot the loss of the model\n",
    "    plt.plot(custom_model.history.history['loss'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">396</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m96\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m396\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">624</span> (2.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m624\u001b[0m (2.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">622</span> (2.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m622\u001b[0m (2.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5211 - loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4089 - loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4120 - loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4109 - loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4114 - loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3949 - loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3928 - loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3711 - loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3953 - loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4050 - loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3890 - loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3854 - loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3911 - loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3977 - loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4136 - loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3877 - loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4024 - loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4048 - loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3896 - loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3904 - loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3952 - loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4070 - loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3978 - loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3923 - loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3955 - loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3988 - loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4043 - loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4016 - loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4054 - loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3886 - loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4190 - loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3938 - loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4127 - loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3993 - loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3928 - loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3974 - loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4041 - loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4018 - loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4064 - loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3840 - loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3963 - loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4056 - loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3862 - loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4057 - loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3759 - loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3884 - loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4070 - loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3962 - loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4019 - loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3890 - loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4083 - loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4181 - loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3970 - loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3853 - loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4020 - loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3856 - loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4066 - loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3892 - loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3913 - loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4042 - loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3795 - loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3988 - loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3820 - loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4025 - loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3959 - loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4067 - loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4042 - loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3922 - loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3928 - loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4123 - loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3861 - loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3990 - loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3840 - loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3968 - loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4024 - loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4022 - loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4053 - loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3958 - loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3998 - loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4165 - loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4060 - loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4040 - loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3976 - loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4061 - loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3856 - loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4015 - loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3998 - loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4008 - loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4052 - loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3846 - loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3941 - loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4036 - loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3996 - loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3908 - loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3933 - loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4022 - loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4068 - loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3806 - loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3890 - loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3844 - loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3455 - loss: 0.0000e+00 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.37603306770324707\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2, but received input with shape (32, 36)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 36), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m custom_model\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m predictions \u001b[38;5;241m=\u001b[39m custom_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# plot the loss of the model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(custom_model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[38], line 72\u001b[0m, in \u001b[0;36mKerasModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Make predictions on the provided data.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    - X: array-like, the data to make predictions on.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[1;32mc:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\tanjo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2, but received input with shape (32, 36)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 36), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Import the data \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv', sep=';')\n",
    "\n",
    "df_enrolled = df[df['Target'] == 'Enrolled']\n",
    "df = df[df['Target'] != 'Enrolled']\n",
    "\n",
    "# Selecting the target\n",
    "y = df['Target']\n",
    "# Change it to binary\n",
    "y = y.apply(lambda x: 1 if x == 'Graduate' else 0)\n",
    "\n",
    "# Selecting the features\n",
    "X = df.drop(columns=['Target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the model parameters\n",
    "    input_dim = X.shape[1]  # Number of input features\n",
    "    layers = [8, 1] # The last one is the number of output classes\n",
    "    activations = ['relu', 'sigmoid']\n",
    "\n",
    "    # Create the custom model\n",
    "    custom_model = KerasModel(input_dim, layers, activations)\n",
    "\n",
    "    # Assume X_train, y_train, X_test, y_test are your data\n",
    "    # Train the model\n",
    "    custom_model.train(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = custom_model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Save the model\n",
    "    # custom_model.save_model(\"custom_model.h5\")\n",
    "\n",
    "    # Load the model\n",
    "    custom_model.load_model(\"custom_model.h5\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = custom_model.predict(X_test)\n",
    "\n",
    "    # plot the loss of the model\n",
    "    plt.plot(custom_model.history.history['loss'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
